library(RColorBrewer)
install.packages("RColorBrewer")
library(RColorBrewer)
brewer.pal(8, "Set2")
brewer.pal(4, "Set2")
cols <- brewer.pal(4, "Set2")
matplot(1:100, r$valid, typ="l", col = c("red", "green", "blue", "black"), xlab="Run", ylab="Validation Set Accuracy")
run <- 2
1:runs
1:run
runs <- 20
?matplot
matplot(1:runs, r$valid, typ=c("l", "l", "l", "l", "l"), col = cols, xlab="Run", ylab="Validation Set Accuracy")
legend("bottomright", inset=.05, legend=c(kernels, "randomForest"), pch=1, col=cols, horiz=FALSE)
View(party.data)
r <- comparison(party.data, runs)
apply(r$test, 2, summary)
?ksvm
4/(13+4)
p <- c(4/(13+4), 0/23)
sum(p*logp)
sum(p*log(p))
p <- c(16/(16+1), 3/23)
sum(p*log(p))
p <- c(16/17, 3/23)
sum(p*log(p))
1-sum(p^2)
p <- c(16/17, 3/23)
sum(p)
sum(p*log(p))
?log
sum(p*log10(p))
p <- c(16/17, 3/23)
-sum(p*log10(p))
1-sum(p^2)
source('~/Dropbox/UCD/DataMining/wk6/entropygini.R', echo=TRUE)
p <- (16/17, 3/23)
p <- c(16/17, 3/23)
entropy(p)
Gini(p)
p <- c(0.52, 0.48)
Gini(p)
entropy(p)
d <- c(17, 4, 0, 19)
p <- d / sum(d)
p
entropy(p)
Gini(p)
source('~/Dropbox/UCD/DataMining/tests/pearson.R', echo=TRUE)
summary(fit)
fit$residuals
fit$deviance
fit$control
install.packages("vcd")
vignette("vcd")
library(vcf)
library(vcd)
vignette("vcd")
?vcd
??vcd
??goodfit
goodfit(fit)
fit$fitted.values
pred <- predict(fit)
pred
pred <- predict(fit, type = "response")
pred
residuals(fit)
residuals(fit, type = "Pearson")
residuals(fit, type = "earson")
residuals(fit, type = "pearson")
??rms
library(rms)
install.packages("rms")
??rms
residuals
residuals(fit, type="pearson")
View(birthwt)
pred
rm(pred)
p <- predict(fit)
p
?predict
prop <- exp(pred) / (1 + exp(pred))
prop <- exp(pred) / (1 + exp(p))
prop <- exp(p) / (1 + exp(p))
prop
precict(fit, type="response")
prop2 <- predcict(fit, type="response")
prop2 <- predict(fit, type="response")
predlow <- pop < 0.5
predlow <- prop < 0.5
predlow
predlow <- as.numeric(predlow)
library(MASS)
source('~/Dropbox/UCD/DataMining/tests/pearson.R', echo=TRUE)
p <- predict(fit)
pred.low <- which(p > 0.5)
pred.low <- which(as.vector(p > 0.5)
)
pred.low <- which(as.vector(p > 0.5))
pred.low
pred.low <- which(as.vector(p >= 0.5))
pred.low
pred.result <- rep(0, lenght(birthwt$low))
pred.result <- rep(0, length(birthwt$low))
pred.result[pred.low] <- 1
pred.result
library(MASS)
data(birthwt)
# Code categorial variable as factors.
birthwt$race<-as.factor(birthwt$race)
birthwt$smoke<-as.factor(birthwt$smoke)
birthwt$ht<-as.factor(birthwt$ht)
birthwt$ui<-as.factor(birthwt$ui)
# Fit the logistic regression model.
fit<-glm(low ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data = birthwt,
family = "binomial")
p <- predict(fit)
# Probability >= 0.5 means low is 1.
pred.low <- which(as.vector(p >= 0.5))
# Predict
predicted.low <- rep(0, lenght(birthwt$low))
# Associate 1 with high probability.
predicted.low[pred.low] <- 1
birthwt$predicted.low <- predicted.low
predicted.low <- rep(0, lenght(birthwt$low))
source('~/Dropbox/UCD/DataMining/tests/pearson.R', echo=TRUE)
View(birthwt)
summary(fit)
?X2GOFtest
??X2GOFtest
library(ROCT)
library(ROCR)
install.packages("ROCT")
install.packages("ROCR")
library(ROCR)
perf <- performance(fit, "tpr", "fpr")
perf <- performance(p, "tpr", "fpr")
predict <- predict(fit, type="response")
source('~/Dropbox/UCD/DataMining/tests/pearson.R', echo=TRUE)
perf
perf@x.name
perf@x.values
?residuals
plot(predict(fit,type="response"),residuals(fit,type="deviance"))
plot(predict(fit,type="response"),residuals(fit,type="pearson"))
residuals(fit,type="pearson")
max(residuals(fit,type="pearson"))
max(residuals(fit,type="pearson"), 3)
max(residuals(fit,type="pearson"))
max.pearson <- max(residuals(fit,type="pearson"))
pearson.resid <- residuals(fit,type="pearson")
max.pearson <- max(pearson.resid)
max.pearson.idx <- which(pearson.resid == max.pearson)
p[155]
birthwt$low[155]
birthwt$low[155]
birthwt$predicted.low
birthwt$predicted.low[155]
# Just some tests in relation to Pearson residuals.
# 06052015
library(MASS)
library(ROCR)
data(birthwt)
# Code categorial variable as factors.
birthwt$race<-as.factor(birthwt$race)
birthwt$smoke<-as.factor(birthwt$smoke)
birthwt$ht<-as.factor(birthwt$ht)
birthwt$ui<-as.factor(birthwt$ui)
# Fit the logistic regression model.
fit<-glm(low ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data = birthwt,
family = "binomial")
p <- predict(fit, type="response")
predobj<-prediction(p,birthwt$low)
# Performance measures.
perf <- performance(predobj,"tpr","fpr")
# Probability >= 0.5 means low is 1.
pred.low <- which(as.vector(p >= 0.5))
# Prediction results.
predicted.low <- rep(0, length(birthwt$low))
# Associate 1 with high probability.
predicted.low[pred.low] <- 1
birthwt$predicted.low <- predicted.low
pearson.resid <- residuals(fit, type="pearson")
max.pearson <- max(pearson.resid)
max.pearson.idx <- which(pearson.resid == max.pearson)
p[max.pearson.idx]
birthwt$low[max.pearson.idx]
birthwt$predicted.low[max.pearson.idx]
# Flip 0's and 1's.
source('~/Dropbox/UCD/DataMining/tests/pearson.R', echo=TRUE)
library(MASS)
library(ROCR)
data(birthwt)
# Code categorial variable as factors.
birthwt$race<-as.factor(birthwt$race)
birthwt$smoke<-as.factor(birthwt$smoke)
birthwt$ht<-as.factor(birthwt$ht)
birthwt$ui<-as.factor(birthwt$ui)
# Flip 0's and 1's.
birthwt$low[birthwt$low == 1] <- 0
birthwt$low[birthwt$low == 0] <- 1
View(birthwt)
library(MASS)
library(ROCR)
data(birthwt)
# Code categorial variable as factors.
birthwt$race<-as.factor(birthwt$race)
birthwt$smoke<-as.factor(birthwt$smoke)
birthwt$ht<-as.factor(birthwt$ht)
birthwt$ui<-as.factor(birthwt$ui)
View(birthwt)
library(MASS)
library(ROCR)
data(birthwt)
# Code categorial variable as factors.
birthwt$race<-as.factor(birthwt$race)
birthwt$smoke<-as.factor(birthwt$smoke)
birthwt$ht<-as.factor(birthwt$ht)
birthwt$ui<-as.factor(birthwt$ui)
# Flip 0's and 1's.
bnew <- birthwt
bnew[birthwt$low == 1] <- 0
bnew[birthwt$low == 0] <- 1
library(caret)
install.packages("caret")
vignette("caret")
training.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-training.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
test.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-testing.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
View(test.raw)
names(test.raw)
test <- test.raw[, -"problem_id"]
test <- test.raw[, -c("problem_id")]
names(test.raw)
test.colnames <-names(test.raw)
train.colnames <- names(train.raw)
train.colnames <- names(training.raw)
train.colnames == test.colnames
which(train.colnames != test.colnames)
train.colnames[, 159]
train.colnames[159]
training.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-training.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
test.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-testing.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
# Pre-processing
# Drop cols 1-6 and all other colls with only NA values in the test set.
drop.cols <- c(1:6,  which(apply(is.na(test.raw), 2, all)==TRUE))
# Drop unneccessary columns and transform variables to numberic.
training <- training.raw[, -drop.cols]
t <- apply(training[, 1:48], 2, as.numeric)
training <- data.frame(t, classe=training[, 49])
library(randomForest)
in.train <- createDataPartition(y=training$classe, p=0.1, list=FALSE)
training.train <- training[in.train, ]
training.test <- training[-in.train, ]
library(caret)
in.train <- createDataPartition(y=training$classe, p=0.1, list=FALSE)
training.train <- training[in.train, ]
training.test <- training[-in.train, ]
rf.model <- randomForest(classe ~ ., data = training, subset = in.train)
rf.model
in.train <- createDataPartition(y=training$classe, p=0.9, list=FALSE)
training.train <- training[in.train, ]
training.test <- training[-in.train, ]
# Build random forest model.
#rf.model <- train(training.train$classe ~ ., method="rf", data=training.train)
rf.model <- randomForest(classe ~ ., data = training, subset = in.train)
rf.model
varImp(rf.model)
varImp
?varImp
str(training.raw$classe)
str(test.raw$classe)
test <- test.raw[, -159]
predictions <- predict(rf.model, newdata = test)
View(test)
test <- test.raw[, -drop.cols]
View(test)
test <- test.raw[, -c(drop.cols, 159)]
View(test)
View(training)
training.cols <- names(training)
test.cols <- names(test)
which(training.cols != test.cols)
training.cols
test.cols
training.cols==test.cols
test.cols[, 49]
test.cols[49
]
test.cols[49]
training.cols[49]
View(test.raw)
test.raw[,49]
rm(test.cols)
rm(train.co)
rm(train.cols)
rm(training.cols)
test.cols <- names(test.raw)
train.cols <- names(training.raw)
which(train.cols != test.cols)
drop.cols
train.cols == test.cols
training.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-training.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
test.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-testing.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
drop.cols <- c(1:6,  which(apply(is.na(test.raw), 2, all)==TRUE))
training <- training.raw[, -drop.cols]
View(training)
str(training)
t <- apply(training[, 1:52], 2, as.numeric)
# activity_prediction.R - For machine learning course submision.
training <- data.frame(t, classe=training[, 53])
test.cols <- names(test.raw)
training <- data.frame(t, classe=training[, 53])
View(training)
test <- test.raw[, -drop.cols]
View(test)
test.cols <- names(test)
train.cols <- names(training)
test.cols==train.cols
rf.model <- randomForest(classe ~ ., data = training, subset = in.train)
View(test)
str(test)
training.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-training.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
test.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-testing.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
drop.cols <- c(1:6,  which(apply(is.na(test.raw), 2, all)==TRUE))
# Drop unneccessary columns and transform variables to numberic.
training <- training.raw[, -drop.cols]
library(caret)
library(randomForest)
in.train <- createDataPartition(y=training$classe, p=0.1, list=FALSE)
training.train <- training[in.train, ]
training.test <- training[-in.train, ]
rf.model <- randomForest(classe ~ ., data = training, subset = in.train)
#
# Drop unneccessary columns and transform variables to numberic.
#
function preprocess_data(in.df, drop.cols) {
out.df <- in.df[, -drop.cols]
t <- apply(out.df[, 1:52], 2, as.numeric)
out.df <- data.frame(t, classe=training[, 53])
return(out.df)
}
function preprocess_data(in.df, drop.cols) {
out.df <- in.df[, -drop.cols]
t <- apply(out.df[, 1:52], 2, as.numeric)
out.df <- data.frame(t, classe=training[, 53])
return(out.df)
}
preprocess_data <- function(in.df, drop.cols) {
out.df <- in.df[, -drop.cols]
t <- apply(out.df[, 1:52], 2, as.numeric)
out.df <- data.frame(t, classe=training[, 53])
return(out.df)
}
training.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-training.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
test.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-testing.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
drop.cols <- c(1:6,  which(apply(is.na(test.raw), 2, all)==TRUE))
training <- preprocess_data(training.raw, drop.cols)
preprocess_data <- function(in.df, drop.cols) {
out.df <- in.df[, -drop.cols]
t <- apply(out.df[, 1:52], 2, as.numeric)
out.df <- data.frame(t, classe=in.df[, 53])
return(out.df)
}
# activity_prediction.R - For machine learning course submision.
drop.cols <- c(1:6,  which(apply(is.na(test.raw), 2, all)==TRUE))
training <- preprocess_data(training.raw, drop.cols)
training.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-training.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
test.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-testing.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
drop.cols <- c(1:6,  which(apply(is.na(test.raw), 2, all)==TRUE))
training <- preprocess_data(training.raw, drop.cols)
View(training)
preprocess_data <- function(in.df, drop.cols) {
out.df <- in.df[, -drop.cols]
t <- apply(out.df[, 1:52], 2, as.numeric)
out.df <- data.frame(t, classe=out.df[, 53])
return(out.df)
}
training.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-training.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
test.raw <- read.csv2('~/Dropbox/coursera/data_science/machine_learning/assignment/data/pml-testing.csv', sep=',', na.strings=c("#DIV/0!", "", "NA"), row.names = 1)
drop.cols <- c(1:6,  which(apply(is.na(test.raw), 2, all)==TRUE))
training <- preprocess_data(training.raw, drop.cols)
View(training)
library(caret)
library(randomForest)
in.train <- createDataPartition(y=training$classe, p=0.1, list=FALSE)
training.train <- training[in.train, ]
training.test <- training[-in.train, ]
rf.model <- randomForest(classe ~ ., data = training, subset = in.train)
test <- preprocess_data(test.raw, drop.cols)
View(test)
test <- preprocess_data(test.raw, drop.cols)
View(test.raw)
test <- preprocess_data(test.raw, drop.cols)
View(test)
test <- test[, -53]
View(test)
predictions <- predict(rf.model, newdata = test)
print(predictions)
source('~/Dropbox/coursera/data_science/machine_learning/assignment/activity_prediction.R', echo=TRUE)
confusionMatrix(predictions.verif, training.verify)
confusionMatrix(predictions.verif, training.verify$classe)
confusionMatrix(predictions.verif, training.verify$classe)
source('~/Dropbox/coursera/data_science/machine_learning/assignment/activity_prediction.R', echo=TRUE)
predictions.full
predictions.full==predictions.check
source('~/Dropbox/coursera/data_science/machine_learning/assignment/activity_prediction.R', echo=TRUE)
predictions.full
predictions.check
predictions.check==predictions.full
predictions.verif
?performance
library(rocr)
install.packages("rocr")
install.packages("ROCR")
library(ROCR)
performance(predictions.full)
performance(predictions.full, "auc")
?prediction
pf <- prediction(predictions.full)
args(trainControl)
hist(training$classe)
table(training$classe)
?trainControl
?train
fitControl <- trainControl(method = "cv",
number = 10)
rf.model.new <- train(classe ~ ., data = training, method="rf", trControl = fitControl)
fitControl <- trainControl(method = "cv",
number = 10,
verboseIter = TRUE)
rf.model.new <- train(classe ~ ., data = training, method="rf", trControl = fitControl)
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1,
verboseIter = TRUE)
rf.model.new <- train(classe ~ ., data = training, method="rf", trControl = fitControl)
train
train.
args(trainControl)
expand.grid(mtry)
trainControl
?trainControl
?train
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1,
verboseIter = TRUE)
rf.model.new <- train(classe ~ ., data = training, method="rf", trControl = fitControl,
tuneLength = 0)
# 10-fold CV
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1,
verboseIter = TRUE)
rf.model.new <- train(classe ~ ., data = training, method="rf", trControl = fitControl,
tuneLength = 3)
# 10-fold CV
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1,
verboseIter = TRUE)
rf.model.new <- train(classe ~ ., data = training, method="rf", trControl = fitControl,
tuneLength = 1)
226/60
source('~/Dropbox/UCD/DataProgC/wk2/classes.R', echo=TRUE)
source('~/Dropbox/UCD/DataProgC/wk2/classes.R', echo=TRUE)
Rcpp::sourceCpp('Dropbox/UCD/DataProgC/lab2/lab3.cpp')
cube
Rcpp::sourceCpp('Dropbox/UCD/DataProgC/lab2/lab3b.cpp')
Rcpp::sourceCpp('Dropbox/UCD/DataProgC/lab2/lab3b.cpp')
source('~/Temp/test.R', echo=TRUE)
hist(sprint, planned)
barplot(sprint, planned)
barplo(sprint, planned)
source('~/Temp/test.R', echo=TRUE)
hist(sprint, planned)
source('~/Temp/test.R', echo=TRUE)
source('~/Temp/test.R', echo=TRUE)
source('~/Temp/test.R', echo=TRUE)
View(data)
?aes
library(ggplot2)
?aes
source('~/Temp/test.R', echo=TRUE)
source('~/Temp/test.R', echo=TRUE)
source('~/Temp/test.R', echo=TRUE)
source('~/Temp/test.R', echo=TRUE)
99%%1
99%%99
99%99
99 % 99
99 %% 99
98 %% 99
-25 %% 3
source('~/Temp/test.R', echo=TRUE)
install.packages(jpeg)
install.packages("jpeg")
install.packages("BayesRCA")
install.packages("BayesLCA")
install.packagegs("arules")
install.packages("arules")
install.packages("arules")
install.packages("arulesViz")
install.packages("LifteTables")
install.packages("LifeTables")
install.packages("cluster")
install.packages("fpc")
install.packages("randomForest")
install.packages("e1071")
setwd("~/Develop/data_analyses/clustering")
